import streamlit as st
st.set_page_config(page_title="AICOSS Chatbot (AICOSS ì±—ë´‡)", layout="wide")

import os
import json
import time
import numpy as np
import faiss
import re
from sentence_transformers import SentenceTransformer
from transformers import MarianMTModel, MarianTokenizer
import ollama
from PIL import Image

# ========================================
# [ì¶”ê°€] íŒŒì¼ íŒŒì‹±ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
# ========================================
import PyPDF2
from docx import Document
from pptx import Presentation
import tempfile

import warnings
warnings.filterwarnings("ignore")

# =====================================================
# 1. ê¸°ë³¸ ì„¤ì •: ê²½ë¡œ ë° í•™êµ ë³„ì¹­, ê¸°íƒ€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# =====================================================
base_dir = r"E:\deep_learning\THU\Capstone\7univ\02057\Capstone"  # ë³¸ì¸ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •
combined_data_path = os.path.join(base_dir, "combined_data.json")
faiss_index_path = os.path.join(base_dir, "faiss_index.bin")

# í•™êµë³„ ë³„ì¹­(ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ì€ ëª¨ë‘ raw ë¬¸ìì—´ë¡œ)
school_aliases = {
    "jju": [
        r"ì „\s*ì£¼\s*ëŒ€í•™êµ", r"ì „\s*ì£¼\s*ëŒ€", r"ì „\s*ëŒ€", r"ì „\s*ì£¼\s*ëŒ€í•™",
        r"jeonju\s*uni", r"jeonju\s*university", r"JJU"
    ],
    "jnu": [
        r"ì „\s*ë‚¨\s*ëŒ€í•™êµ", r"ì „\s*ë‚¨\s*ëŒ€", r"ì „\s*ë‚¨\s*ëŒ€í•™", r"ì „ë‚¨ëŒ€",
        r"jeonnam\s*uni", r"jeonnam\s*university", r"jeonnam\s*national\s*university", r"JNU"
    ],
    "knu": [
        r"ê²½\s*ë¶\s*ëŒ€í•™êµ", r"ê²½\s*ë¶\s*ëŒ€", r"ê²½\s*ë¶\s*ëŒ€í•™", r"ê²½ë¶ëŒ€",
        r"kyungpook\s*uni", r"kyungpook\s*university", r"kyungpook\s*national\s*university", r"KNU"
    ],
    "skku": [
        r"ì„±\s*ê· \s*ê´€\s*ëŒ€í•™êµ", r"ì„±\s*ê· \s*ê´€\s*ëŒ€", r"ì„±\s*ëŒ€", r"ì„±ê· ê´€ëŒ€",
        r"sungkyunkwan\s*uni", r"sungkyunkwan\s*university", r"SKKU"
    ],
    "tec": [
        r"ì„œ\s*ìš¸\s*ê³¼\s*í•™\s*ê¸°\s*ìˆ \s*ëŒ€í•™êµ", r"ì„œ\s*ìš¸\s*ê³¼\s*í•™\s*ê¸°\s*ìˆ \s*ëŒ€",
        r"ê³¼\s*í•™\s*ê¸°\s*ìˆ \s*ëŒ€í•™êµ", r"ê³¼\s*í•™\s*ê¸°\s*ìˆ \s*ëŒ€",
        r"ê³¼\s*ê¸°\s*ëŒ€í•™êµ", r"ê³¼\s*ê¸°\s*ëŒ€", r"ì„œìš¸ê³¼ê¸°ëŒ€", r"SeoulTech",
        r"Seoul\s*National\s*University\s*of\s*Science\s*and\s*Technology",
        r"Seoul\s*National\s*University", r"TEC"
    ],
    "uos": [
        r"ì„œ\s*ìš¸\s*ì‹œ\s*ë¦½\s*ëŒ€í•™êµ", r"ì„œ\s*ìš¸\s*ì‹œ\s*ë¦½\s*ëŒ€", r"ì‹œë¦½ëŒ€",
        r"uni\s*of\s*Seoul", r"University\s*of\s*Seoul", r"UOS"
    ],
    "yj": [
        r"ì˜\s*ì§„\s*ì „\s*ë¬¸\s*ëŒ€í•™êµ", r"ì˜\s*ì§„\s*ì „\s*ë¬¸\s*ëŒ€", r"ì˜ì§„ëŒ€", r"ì˜ì§„ì „ë¬¸ëŒ€",
        r"Yeungjin\s*University", r"Yeungjin\s*uni", r"YJU"
    ]
}

def normalize_school_names(text: str) -> str:
    """í•™êµ ì´ë¦„ì˜ ë‹¤ì–‘í•œ ë³€í˜•ì„ í‘œì¤€ í‚¤ë¡œ ì¹˜í™˜í•©ë‹ˆë‹¤."""
    for standard, aliases in school_aliases.items():
        for alias in aliases:
            text = re.sub(alias, standard, text, flags=re.IGNORECASE)
    return text

def exclude_school_names_during_translation(text: str):
    """ë²ˆì—­ ì „ í•™êµ ì´ë¦„ì„ ë³´í˜¸í•˜ì—¬ ë²ˆì—­ë˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤."""
    protected_names = {}
    for standard, aliases in school_aliases.items():
        for alias in aliases:
            matches = re.findall(alias, text, flags=re.IGNORECASE)
            for match in matches:
                if match not in protected_names.values():
                    placeholder = f"[[{standard}]]"
                    protected_names[placeholder] = match
                    text = text.replace(match, placeholder)
    return text, protected_names

def restore_protected_names(text: str, protected_names: dict) -> str:
    """ë²ˆì—­ í›„ ë³´í˜¸ëœ í•™êµ ì´ë¦„ì„ ì›ë˜ëŒ€ë¡œ ë³µì›í•©ë‹ˆë‹¤."""
    for placeholder, original in protected_names.items():
        text = text.replace(placeholder, original)
    return text

def get_school_key(school_name: str) -> str:
    """
    ì…ë ¥ëœ í•™êµ ì´ë¦„ ë¬¸ìì—´ì´ school_aliasesì— ì •ì˜ëœ íŒ¨í„´ê³¼ ë§¤ì¹­ë˜ë©´,
    í•´ë‹¹ í‘œì¤€ í‚¤(ì˜ˆ: "jju")ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. ë§¤ì¹­ë˜ì§€ ì•Šìœ¼ë©´ ì›ë˜ ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    for key, alias_list in school_aliases.items():
        if school_name.lower() == key.lower():
            return key
        for alias in alias_list:
            if re.fullmatch(alias, school_name, flags=re.IGNORECASE):
                return key
    return school_name

# =====================================================
# 2. ëª¨ë¸ ë° ë°ì´í„° ë¡œë”© (st.cache_resource ì‚¬ìš©)
# =====================================================
@st.cache_resource
def load_all_entries():
    with open(combined_data_path, 'r', encoding='utf-8') as f:
        combined_data = json.load(f)
    all_entries = combined_data.get("data_entries", []) + combined_data.get("image_entries", [])
    return all_entries

@st.cache_resource
def load_faiss_index():
    index = faiss.read_index(faiss_index_path)
    st.write(f"Loaded FAISS index, total vectors: {index.ntotal} (ë¶ˆëŸ¬ì˜¨ FAISS ì¸ë±ìŠ¤, ì´ ë²¡í„° ìˆ˜: {index.ntotal})")
    return index

@st.cache_resource
def load_translation_model():
    model_name = "Helsinki-NLP/opus-mt-ko-en"
    tokenizer = MarianTokenizer.from_pretrained(model_name)
    model = MarianMTModel.from_pretrained(model_name)
    return tokenizer, model

@st.cache_resource
def load_embedding_model():
    model = SentenceTransformer('all-MiniLM-L6-v2')
    return model

# ì „ì—­ ë³€ìˆ˜ ë¡œë”©
all_entries = load_all_entries()
faiss_index = load_faiss_index()
trans_tokenizer, trans_model = load_translation_model()
embedding_model = load_embedding_model()

# =====================================================
# 3. ë²ˆì—­ ë° ì„ë² ë”© ê´€ë ¨ í•¨ìˆ˜
# =====================================================
def translate_ko_to_en(text: str, tokenizer, model) -> str:
    """
    í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­í•˜ë˜, í•™êµ ì´ë¦„(ì •ê·œì‹ì— ë§ëŠ” ë¶€ë¶„)ì€ ë²ˆì—­í•˜ì§€ ì•Šê³ 
    school_aliasesì— ì •ì˜ëœ í‚¤ ê°’(ì˜ˆ: "jju")ìœ¼ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
    """
    patterns = []
    for key, alias_list in school_aliases.items():
        patterns.append(re.escape(key))
        patterns.extend(alias_list)
    combined_pattern = "(" + "|".join(patterns) + ")"
    pattern_re = re.compile(combined_pattern, flags=re.IGNORECASE)

    parts = re.split(pattern_re, text)
    translated_parts = []
    for i, part in enumerate(parts):
        if i % 2 == 1:
            # í•™êµ ì´ë¦„(í˜¹ì€ ë³„ì¹­) ë¶€ë¶„ì´ë©´ í‚¤ë¡œ normalize
            normalized_key = get_school_key(part)
            translated_parts.append(normalized_key)
        else:
            if part.strip() == "":
                translated_parts.append(part)
            else:
                inputs = tokenizer(part, return_tensors="pt", padding=True, truncation=True)
                translated = model.generate(**inputs)
                translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)
                translated_parts.append(translated_text)
    return "".join(translated_parts)

def get_embedding(text: str):
    emb = embedding_model.encode(text, convert_to_numpy=True)
    return emb.astype('float32')

def detect_language(text: str) -> str:
    hangul_count = sum(1 for ch in text if 'ê°€' <= ch <= 'í£')
    english_count = sum(1 for ch in text if ch.lower() in 'abcdefghijklmnopqrstuvwxyz')
    return 'ko' if hangul_count >= english_count else 'en'

# =====================================================
# 4. ê²€ìƒ‰ ë° Q&A ê´€ë ¨ í•¨ìˆ˜
# =====================================================
def get_university_from_question(question: str):
    """
    ì§ˆë¬¸ì—ì„œ í•™êµ ì´ë¦„(ëŒ€í•™ ëª…ì¹­)ì„ ì°¾ìŠµë‹ˆë‹¤.
    """
    for standard, aliases in school_aliases.items():
        for alias in aliases:
            if re.search(alias, question, flags=re.IGNORECASE):
                return standard
    return None

def retrieve_best_matches(query: str, lang: str, top_k: int = 3):
    """
    ë‚´ë¶€ DBì—ì„œ ê²€ìƒ‰í•´ì˜¬ ë•Œ ì‚¬ìš©.
    """
    university_key = get_university_from_question(query)
    if university_key is None:
        return None, [], "âš  Unable to find university name in question (âš  ì§ˆë¬¸ì—ì„œ ëŒ€í•™ ì´ë¦„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.)"
    
    filtered_entries = [entry for entry in all_entries if entry.get("university") == university_key]
    if not filtered_entries:
        return None, [], f"âš  No data available for {university_key} (âš  {university_key}ì— ëŒ€í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.)"
    
    index_to_content_kr = {entry["index"]: entry["content_kr"] for entry in filtered_entries if "content_kr" in entry}
    index_to_content_en = {entry["index"]: entry["content"] for entry in filtered_entries if "content" in entry}
    index_to_summary = {entry["index"]: entry.get("summary", "(ìš”ì•½ ì—†ìŒ)") for entry in filtered_entries}

    summary_vectors = np.array(
        [embedding_model.encode(index_to_summary[idx]) for idx in index_to_summary.keys()],
        dtype='float32'
    )
    # FAISS ì„œë¸Œ ì¸ë±ìŠ¤
    faiss_subset = faiss.IndexFlatL2(summary_vectors.shape[1])
    faiss_subset.add(summary_vectors)
    valid_indices = list(index_to_summary.keys())

    if lang == 'ko':
        translated_query = translate_ko_to_en(query, trans_tokenizer, trans_model)
        search_query = translated_query
    else:
        translated_query = None
        search_query = normalize_school_names(query)

    query_emb = embedding_model.encode(search_query)
    query_emb = np.expand_dims(query_emb, axis=0)

    distances, subset_indices = faiss_subset.search(query_emb, top_k)
    retrieved_contexts = []
    retrieved_indices = []
    for subset_idx in subset_indices[0]:
        real_idx = valid_indices[subset_idx]
        retrieved_indices.append(real_idx)
        content = index_to_content_kr.get(real_idx) if lang == 'ko' else index_to_content_en.get(real_idx)
        summary = index_to_summary.get(real_idx)
        if content is not None:
            # ì—¬ê¸°ì„œëŠ” content + summaryë¥¼ ë¬¶ì–´ì„œ "reference_info" ë¡œ ì‚¬ìš©
            retrieved_contexts.append(f"{content}\n\nexplanation:\n{summary}")
    full_reference = "\n\n".join(retrieved_contexts)
    return translated_query, retrieved_indices, full_reference

# =====================================================
# === ìƒˆë¡­ê²Œ ë¶„ë¦¬ëœ ë‘ í•¨ìˆ˜: generate_answer_ephemeral / generate_answer_db
# =====================================================
def generate_answer_ephemeral(user_msg: str, conversation_history: str, reference_info: str, lang: str) -> str:
    """
    [íŒŒì¼ ê¸°ë°˜ Q&A]ë¥¼ ìœ„í•œ í•¨ìˆ˜.
    ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ ê°€ì ¸ì˜¨ ì°¸ê³  ì •ë³´ë¥¼ í™œìš©í•´ ë‹µë³€.
    """
    OLLAMA_MODEL = "qwen2.5:1.5b"

    if lang == 'ko':
        prompt = f"""
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í™” ë‚´ì—­ê³¼ ì°¸ê³  ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”. (Answer in KOREAN)
ì •í™•í•œ ì •ë³´ì¸ "ì°¸ê³  ì •ë³´" ì•ˆì— ìˆëŠ” ë‚´ìš©ì„ ì‚¬ìš©í•´ì„œ ë‹µë³€í•˜ì„¸ìš”.
ê±°ì§“ë§ì€ í•  ìˆ˜ ì—†ìœ¼ë©° ì¹œì ˆí•˜ê²Œ ëŒ€ë‹µí•´ì•¼ í•´ìš”.

ëŒ€í•™êµëª…ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, "í•™êµëª…"ì— ìˆëŠ” í•™êµëª…ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.

í•™êµëª…: ì „ì£¼ëŒ€í•™êµ, ì „ë‚¨ëŒ€í•™êµ, ê²½ë¶ëŒ€í•™êµ, ì„±ê· ê´€ëŒ€í•™êµ, ì„œìš¸ê³¼í•™ê¸°ìˆ ëŒ€í•™êµ, ì„œìš¸ì‹œë¦½ëŒ€í•™êµ, ì˜ì§„ì „ë¬¸ëŒ€í•™êµ

[ëŒ€í™” ë‚´ì—­]:
{conversation_history}

[ì°¸ê³  ì •ë³´]:
{reference_info}

ì§ˆë¬¸:
{user_msg}
"""
    else:
        prompt = f"""
[Source: User uploaded file]
Answer the user's question using the provided conversation history and reference information.
Use Reference Information which is trustable to answer.
You can't lie and should answer politely.

When you talk about university, only use University Name in "University Name"

University Name: 
Jeonju University, Jeonnam National University, Kyungpook National University, Sungkyunkwan University, Seoul National University of Science and Technology, University of Seoul, Yeungjin University


Conversation History:
{conversation_history}

Reference Information:
{reference_info}

Question:
{user_msg}
"""

    response = ollama.chat(model=OLLAMA_MODEL, messages=[{"role": "user", "content": prompt}])
    return response["message"]["content"]

def generate_answer_db(user_msg: str, conversation_history: str, reference_info: str, lang: str) -> str:
    """
    [ë‚´ë¶€ DB ê¸°ë°˜ Q&A]ë¥¼ ìœ„í•œ í•¨ìˆ˜.
    ë‚´ë¶€ DBì—ì„œ ê°€ì ¸ì˜¨ ì°¸ê³  ì •ë³´ë¥¼ í™œìš©í•´ ë‹µë³€.
    """
    OLLAMA_MODEL = "qwen2.5:1.5b"

    if lang == 'ko':
        prompt = f"""
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í™” ë‚´ì—­ê³¼ ì°¸ê³  ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”. (Answer in KOREAN)
"ì°¸ê³  ì •ë³´" ì•ˆì— ìˆëŠ” ë‚´ìš©ì„ ì¤‘ì ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”.
ê±°ì§“ë§ì€ í•  ìˆ˜ ì—†ìœ¼ë©° ì¹œì ˆí•˜ê²Œ ëŒ€ë‹µí•´ì•¼ í•´ìš”.

ëŒ€í•™êµëª…ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, "í•™êµëª…"ì— ìˆëŠ” í•™êµëª…ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.

í•™êµëª…: ì „ì£¼ëŒ€í•™êµ, ì „ë‚¨ëŒ€í•™êµ, ê²½ë¶ëŒ€í•™êµ, ì„±ê· ê´€ëŒ€í•™êµ, ì„œìš¸ê³¼í•™ê¸°ìˆ ëŒ€í•™êµ, ì„œìš¸ì‹œë¦½ëŒ€í•™êµ, ì˜ì§„ì „ë¬¸ëŒ€í•™êµ


[ëŒ€í™” ë‚´ì—­]:
{conversation_history}

[ì°¸ê³  ì •ë³´]:
{reference_info}

ì§ˆë¬¸:
{user_msg}
"""
    else:
        prompt = f"""
[Source: Internal DB]
Answer the user's question using the provided conversation history and reference information.

When you talk about university, only use University Name in "University Name"

University Name: 
Jeonju University, Jeonnam National University, Kyungpook National University, Sungkyunkwan University, Seoul National University of Science and Technology, University of Seoul, Yeungjin University

Conversation History:
{conversation_history}

Reference Information:
{reference_info}

Question:
{user_msg}
"""

    response = ollama.chat(model=OLLAMA_MODEL, messages=[{"role": "user", "content": prompt}])
    return response["message"]["content"]

# =====================================================
# 5. ì´ë¯¸ì§€ ê´€ë ¨ í•¨ìˆ˜ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
# =====================================================
def retrieve_best_image(query: str, lang: str):
    """
    í•´ë‹¹ ëŒ€í•™ì— ì†í•˜ëŠ” image_entries ì¤‘ì—ì„œ, ì´ë¯¸ì§€ summaryë¥¼ ì„ë² ë”©í•˜ì—¬
    ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°€ ê°€ì¥ ë†’ì€ ì´ë¯¸ì§€ 1ì¥ì„ ë°˜í™˜ (ì—†ìœ¼ë©´ None)
    """
    university_key = get_university_from_question(query)
    if university_key is None:
        return None, None
    
    filtered_images = [entry for entry in all_entries 
                       if entry.get("university") == university_key 
                       and entry.get("data_type") == "image"]
    if not filtered_images:
        return None, None
    
    index_to_summary = {}
    for entry in filtered_images:
        idx = entry["index"]
        index_to_summary[idx] = entry.get("summary", "(no summary)")

    if lang == 'ko':
        translated_query = translate_ko_to_en(query, trans_tokenizer, trans_model)
        search_query = translated_query
    else:
        search_query = normalize_school_names(query)

    query_emb = embedding_model.encode(search_query)
    query_emb = np.expand_dims(query_emb, axis=0)

    image_vectors = np.array(
        [embedding_model.encode(index_to_summary[i]) for i in index_to_summary.keys()],
        dtype='float32'
    )
    faiss_subset = faiss.IndexFlatL2(image_vectors.shape[1])
    faiss_subset.add(image_vectors)
    valid_indices = list(index_to_summary.keys())

    distances, subset_indices = faiss_subset.search(query_emb, 1)
    best_idx = subset_indices[0][0]
    real_idx = valid_indices[best_idx]
    best_image_entry = None
    for ent in filtered_images:
        if ent["index"] == real_idx:
            best_image_entry = ent
            break

    if best_image_entry is None:
        return None, None

    best_image_summary = best_image_entry.get("summary", "(no summary)")
    return best_image_entry, best_image_summary

def ask_llm_if_use_image(user_msg: str, text_answer: str, image_summary: str) -> str:
    """
    LLMì—ê²Œ ì´ë¯¸ì§€ê°€ ìœ ì €ì˜ ì§ˆë¬¸ ë‹µë³€ì— ê¼­ í•„ìš”í•œì§€ ë¬»ìŠµë‹ˆë‹¤.
    ì‘ë‹µì— "yes"ê°€ í¬í•¨ë˜ë©´ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    OLLAMA_MODEL = "qwen2.5:1.5b"
    prompt = f"""
You must determine whether the given [Image Summary] has similar content or context with the [User Question].

[User Question]
{user_msg}

[Image Summary]
{image_summary}

Answer strictly with "yes" if the [Image Summary] is related to User Question.
Respond with only "yes" or "no".
"""
    response = ollama.chat(model=OLLAMA_MODEL, messages=[{"role": "user", "content": prompt}])
    content = response["message"]["content"].strip()
    return content

def get_relative_image_path(absolute_path: str) -> str:
    """
    ì ˆëŒ€ê²½ë¡œë¥¼ Capstone ì´í›„ í•˜ìœ„ í´ë”ë¶€í„°ì˜ ìƒëŒ€ê²½ë¡œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    splitted = absolute_path.split('Capstone')
    if len(splitted) < 2:
        return os.path.basename(absolute_path)
    rel_part = splitted[-1].replace('\\', '/').lstrip('/')
    return rel_part

# =====================================================
# [ì¶”ê°€ ê¸°ëŠ¥] PDF/Word/PPT í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì„ë² ë”©
# =====================================================
def is_english_paragraph(text: str, threshold: float = 0.7) -> bool:
    """
    ë¬¸ë‹¨ì—ì„œ ì˜ì–´ ì•ŒíŒŒë²³(a-z, A-Z)ì´ ì°¨ì§€í•˜ëŠ” ë¹„ìœ¨ì´ threshold ì´ìƒì¸ì§€ íŒë³„.
    ìˆ«ìë‚˜ ê³µë°±, íŠ¹ìˆ˜ë¬¸ìëŠ” ì œì™¸í•˜ê³  ê³„ì‚°.
    """
    filtered = "".join(ch for ch in text if ch.isalpha())
    if len(filtered) == 0:
        return False
    english_chars = sum(1 for ch in filtered if ('a' <= ch.lower() <= 'z'))
    ratio = english_chars / len(filtered)
    return ratio >= threshold

def parse_pdf(file_content: bytes) -> str:
    """
    PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ (PyPDF2 ì‚¬ìš©).
    """
    text = ""
    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp:
        tmp.write(file_content)
        tmp_path = tmp.name

    with open(tmp_path, 'rb') as f:
        reader = PyPDF2.PdfReader(f)
        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"

    os.remove(tmp_path)
    return text

def parse_docx(file_content: bytes) -> str:
    """
    Word(docx)ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ (python-docx ì‚¬ìš©).
    """
    text = ""
    with tempfile.NamedTemporaryFile(delete=False, suffix='.docx') as tmp:
        tmp.write(file_content)
        tmp_path = tmp.name
    
    doc = Document(tmp_path)
    for para in doc.paragraphs:
        text += para.text + "\n"
    os.remove(tmp_path)
    return text

def parse_pptx(file_content: bytes) -> str:
    """
    PPT(pptx)ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ (python-pptx ì‚¬ìš©).
    """
    text = ""
    with tempfile.NamedTemporaryFile(delete=False, suffix='.pptx') as tmp:
        tmp.write(file_content)
        tmp_path = tmp.name

    prs = Presentation(tmp_path)
    for slide in prs.slides:
        for shape in slide.shapes:
            if hasattr(shape, "text"):
                text += shape.text + "\n"
    os.remove(tmp_path)
    return text

# =====================================================
# [ê°œì„ ëœ] ë¬¸ì„œ ì²­í‚¹ í•¨ìˆ˜ (ë§¥ë½ê¸°ë°˜, ë¬¸ë‹¨+ë¬¸ì¥ í˜¼í•©, ê¸¸ì´ ì œí•œ)
# =====================================================
def chunk_text_smart(text: str, max_chars:int=1500, min_chars:int=700, overlap:int=200) -> list:
    """
    1) ë¹ˆ ì¤„ì„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ë‹¨ì„ ë‚˜ëˆ”
    2) ê° ë¬¸ë‹¨ì´ ë„ˆë¬´ ê¸¸ë©´, ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
    3) í•˜ë‚˜ì˜ ì²­í¬ë¥¼ ë§Œë“¤ ë•Œ, ë¬¸ë‹¨ì„ ì°¨ë¡€ë¡œ ë¶™ì—¬ê°€ë©° 
       - í˜„ì¬ ì²­í¬ ê¸¸ì´ê°€ min_chars ì´í•˜ì¼ ë•ŒëŠ” ê³„ì† ë¶™ì„
       - max_charsë¥¼ ë„˜ê¸°ë©´ ì²­í¬ í™•ì •
    4) ì²­í¬ ê°„ overlap(ë¬¸ì ìˆ˜)ë§Œí¼ ì•ì„  ë‚´ìš©ì—ì„œ ê²¹ì³ì„œ ì¶”ê°€ (ë§¥ë½ ìœ ì§€)
    """

    # 1) ë¹ˆ ì¤„ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ë‹¨ ë‚˜ëˆ„ê¸°
    paragraphs = re.split(r'\n\s*\n', text.strip())
    paragraphs = [p.strip() for p in paragraphs if p.strip()]

    # 2) ë§Œì•½ ì–´ë–¤ ë¬¸ë‹¨ì´ max_charsë³´ë‹¤ í›¨ì”¬ ê¸¸ë©´, ë¬¸ì¥ ë‹¨ìœ„ ì¬ë¶„í• 
    expanded_paragraphs = []
    for para in paragraphs:
        if len(para) > max_chars:
            # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìª¼ê°œì„œ max_chars ì´í•˜ë¡œ ë‚˜ëˆ”
            sentences = re.split(r'(?<=[.?!])\s+', para)
            temp_buf = ""
            for sent in sentences:
                if len(temp_buf) + len(sent) <= max_chars:
                    temp_buf += (sent + " ")
                else:
                    expanded_paragraphs.append(temp_buf.strip())
                    temp_buf = sent + " "
            if temp_buf.strip():
                expanded_paragraphs.append(temp_buf.strip())
        else:
            expanded_paragraphs.append(para)

    # 3) ì²­í¬ ìƒì„±
    chunks = []
    current_chunk = ""
    for para in expanded_paragraphs:
        if not current_chunk:
            # ë¹„ì–´ ìˆìœ¼ë©´ ê·¸ëƒ¥ ì‹œì‘
            current_chunk = para
        else:
            # ì´ì–´ë¶™ì˜€ì„ ë•Œ ê¸¸ì´ í™•ì¸
            if len(current_chunk) + len(para) + 1 <= max_chars:
                current_chunk += "\n" + para
            else:
                # ì¼ë‹¨ í˜„ì¬ ì²­í¬ í™•ì •
                chunks.append(current_chunk.strip())
                current_chunk = para

        # ë§Œì•½ í˜„ì¬ ì²­í¬ê°€ ìµœì†Œ ê¸¸ì´ë¥¼ ë„˜ëŠ” ìƒíƒœì—ì„œ max_chars ê°€ê¹Œì´ ê°”ë‹¤ë©´ ë°”ë¡œ í™•ì •
        if len(current_chunk) > max_chars:
            chunks.append(current_chunk.strip())
            current_chunk = ""

    # ë§ˆì§€ë§‰ ë‚¨ì€ ì²­í¬
    if current_chunk.strip():
        chunks.append(current_chunk.strip())

    # 4) ì˜¤ë²„ë© ì²˜ë¦¬
    if overlap > 0:
        final_chunks = []
        for i, c in enumerate(chunks):
            if i == 0:
                final_chunks.append(c)
            else:
                # ì´ì „ ì²­í¬ ë’¤ overlap ë§Œí¼ ì˜ë¼ì„œ ë¶™ì´ê¸°
                prev_chunk = final_chunks[-1]
                overlap_text = prev_chunk[-overlap:] if len(prev_chunk) > overlap else prev_chunk
                new_chunk = overlap_text + "\n" + c
                final_chunks.append(new_chunk)
        return final_chunks
    else:
        return chunks

def create_ephemeral_faiss_index(chunks: list) -> faiss.IndexFlatL2:
    """
    ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì— ëŒ€í•´ ì„ë² ë”©ì„ ê³„ì‚°í•˜ê³  FAISS ì¸ë±ìŠ¤ë¥¼ ë§Œë“¦.
    """
    embeddings = [embedding_model.encode(c) for c in chunks]
    embeddings = np.array(embeddings, dtype='float32')
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)
    return index

def ephemeral_retrieve_query(query: str, chunks: list, faiss_index: faiss.IndexFlatL2, top_k: int = 3) -> str:
    """
    ì—…ë¡œë“œëœ íŒŒì¼ ì²­í¬ì— ëŒ€í•´ì„œë§Œ ì§ˆë¬¸(query)ì„ ê²€ìƒ‰í•˜ê³  ìƒìœ„ top_kê°œë¥¼ ë°˜í™˜.
    """
    query_emb = embedding_model.encode(query)
    query_emb = np.expand_dims(query_emb, axis=0)
    distances, indices = faiss_index.search(query_emb, top_k)

    retrieved_texts = []
    for idx in indices[0]:
        if 0 <= idx < len(chunks):
            retrieved_texts.append(chunks[idx])
    full_reference = "\n\n".join(retrieved_texts)
    return full_reference

# =====================================================
# 7. Streamlit UI ë©”ì¸
# =====================================================
st.title("AICOSS ChatbotğŸ¤–")

# ëŒ€í™” ë‚´ì—­ì€ session_stateì— ì €ì¥
if "conversation_history" not in st.session_state:
    st.session_state.conversation_history = ""

# [ì¶”ê°€] íŒŒì¼ ì—…ë¡œë” (PDF, Word, PPT) - ìµœëŒ€ 1MB, ì˜ì–´ë§Œ ê°€ëŠ¥
uploaded_file = st.file_uploader(
    "Upload a PDF/Word/PPT file (Max:1MB, English only)",
    type=["pdf", "docx", "pptx"],
    help="Only English documents are supported. Max file size = 1MB."
)

with st.form("chat_form", clear_on_submit=True):
    user_input = st.text_input("Enter your question (ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”)")
    submitted = st.form_submit_button("Send (ì „ì†¡)")

# [ì¶”ê°€] ì—…ë¡œë“œëœ íŒŒì¼ì„ ì²˜ë¦¬í•  ì„ì‹œ ë³€ìˆ˜
file_chunks = None
ephemeral_index = None

if uploaded_file:
    if uploaded_file.size > 1 * 1024 * 1024:
        st.warning("File size exceeds 1MB limit. The file will be ignored. (1MB ì´ˆê³¼ íŒŒì¼ì€ ë¬´ì‹œë©ë‹ˆë‹¤.)")
    else:
        # íŒŒì¼ íŒŒì‹±
        file_type = uploaded_file.name.split(".")[-1].lower()
        file_text = ""
        if file_type == "pdf":
            file_text = parse_pdf(uploaded_file.read())
        elif file_type == "docx":
            file_text = parse_docx(uploaded_file.read())
        elif file_type == "pptx":
            file_text = parse_pptx(uploaded_file.read())

        # ë¬¸ë‹¨(ì¤„ë°”ê¿ˆ ê¸°ì¤€) ë‹¨ìœ„ë¡œ ì˜ì–´ë¬¸ë‹¨ì¸ì§€ í™•ì¸í•˜ì—¬ í•„í„°ë§
        paragraphs = file_text.split("\n")
        english_paragraphs = [p for p in paragraphs if is_english_paragraph(p.strip())]
        if not english_paragraphs:
            st.warning("No English paragraphs found or less than 70% English text. (ì˜ì–´ ë¬¸ë‹¨ì´ ì¶©ë¶„ì¹˜ ì•ŠìŠµë‹ˆë‹¤.)")
        else:
            # ìœ íš¨í•œ ë¬¸ë‹¨ë§Œ ëª¨ì•„ í° í…ìŠ¤íŠ¸ë¡œ ë§Œë“  ë’¤, ê°œì„ ëœ ì²­í‚¹ í•¨ìˆ˜ ì‚¬ìš©
            filtered_text = "\n\n".join(english_paragraphs)
            # ì•„ë˜ íŒŒë¼ë¯¸í„°ë“¤ì€ ìƒí™©ì— ë§ê²Œ ì¡°ì ˆ ê°€ëŠ¥
            file_chunks = chunk_text_smart(filtered_text, max_chars=1500, min_chars=700, overlap=200)
            ephemeral_index = create_ephemeral_faiss_index(file_chunks)

if submitted and user_input.strip():
    lang = detect_language(user_input)

    # [íŒŒì¼ì´ ì—…ë¡œë“œëœ ê²½ìš°] => ì—…ë¡œë“œ íŒŒì¼ ê¸°ë°˜ Q&A
    if file_chunks and ephemeral_index:
        with st.spinner("Searching your uploaded file..."):
            # íŒŒì¼ ì§ˆì˜ ì‹œ, ì˜ì–´ ì¿¼ë¦¬ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜ / í•œêµ­ì–´ë©´ ì˜ì–´ë¡œ ë²ˆì—­
            if lang == 'ko':
                search_query = translate_ko_to_en(user_input, trans_tokenizer, trans_model)
            else:
                search_query = user_input  # ì˜ì–´ë¼ë©´ ê·¸ëŒ€ë¡œ
            reference_info = ephemeral_retrieve_query(search_query, file_chunks, ephemeral_index, top_k=5)

        if not reference_info.strip():
            st.warning("âš  Could not find relevant info in the uploaded file.")
        else:
            with st.spinner("Generating answer from your file..."):
                # íŒŒì¼ ê¸°ë°˜ Q&A
                answer = generate_answer_ephemeral(
                    user_input,
                    st.session_state.conversation_history,
                    reference_info,
                    lang
                )

            # ìŠ¤íŠ¸ë¦¬ë° í˜•íƒœ ì¶œë ¥
            answer_placeholder = st.empty()
            streamed_answer = ""
            for char in answer:
                streamed_answer += char
                answer_placeholder.markdown(streamed_answer)
                time.sleep(0.02)

            # ì‚¬ìš©ëœ ì°¸ê³  ì •ë³´ **ì›ë³¸ ê·¸ëŒ€ë¡œ** í‘œì‹œ
            st.markdown("### Used Retrieved Information (ì‚¬ìš©ëœ ì°¸ê³  ì •ë³´)")
            st.markdown(reference_info)

            # ëŒ€í™” ë‚´ì—­ ì—…ë°ì´íŠ¸
            st.session_state.conversation_history += f"User: {user_input}\nBot: {answer}\n\n"
            st.markdown("### Conversation History (ëŒ€í™” ë‚´ì—­)")
            st.text_area("", st.session_state.conversation_history, height=200)

    else:
        # [íŒŒì¼ì´ ì—†ê±°ë‚˜(í¬ê¸° ì´ˆê³¼, ì˜ì–´ ì•„ë‹˜, ë“±) -> DB ê¸°ë°˜ Q&A]
        with st.spinner("Searching the internal DB..."):
            translated_query, indices, reference_info = retrieve_best_matches(user_input, lang, top_k=3)

        if not reference_info.strip():
            st.warning("âš  Unable to find relevant information from DB.")
        else:
            with st.spinner("Generating answer from internal DB..."):
                answer = generate_answer_db(
                    user_input,
                    st.session_state.conversation_history,
                    reference_info,
                    lang
                )

            # ì´ë¯¸ì§€ ê²€ìƒ‰
            image_entry, image_summary = retrieve_best_image(user_input, lang)
            use_image = False
            relative_image_path = None
            if image_entry and image_summary:
                yes_no = ask_llm_if_use_image(user_input, answer, image_summary)
                if "yes" in yes_no.lower():
                    use_image = True
                    absolute_image_path = image_entry["file_path"]
                    relative_image_path = get_relative_image_path(absolute_image_path)

            # ì´ë¯¸ì§€ê°€ ì‚¬ìš©ë  ê²½ìš° í‘œì‹œ
            if use_image and relative_image_path:
                image_absolute_path = os.path.join(base_dir, relative_image_path.replace('/', os.sep))
                try:
                    img = Image.open(image_absolute_path)
                    st.image(img, caption="Image (ì´ë¯¸ì§€)", use_column_width=True)
                except Exception as e:
                    st.error(f"Failed to display image: {e}")

            # ë‹µë³€ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥
            answer_placeholder = st.empty()
            streamed_answer = ""
            for char in answer:
                streamed_answer += char
                answer_placeholder.markdown(streamed_answer)
                time.sleep(0.02)

            # ì‚¬ìš©ëœ ì°¸ê³  ì •ë³´ **ì›ë³¸ ê·¸ëŒ€ë¡œ** í‘œì‹œ
            st.markdown("### Used Retrieved Information (ì‚¬ìš©ëœ ì°¸ê³  ì •ë³´)")
            st.markdown(reference_info)

            # ëŒ€í™” ë‚´ì—­ ì—…ë°ì´íŠ¸
            st.session_state.conversation_history += f"User: {user_input}\nBot: {answer}\n\n"
            st.markdown("### Conversation History (ëŒ€í™” ë‚´ì—­)")
            st.text_area("", st.session_state.conversation_history, height=200)
